{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "Supervised learning is where you have input variables (X) and an output variable (y) and you use an algorithm to learn the mapping function from the input to the output.\n",
    "\n",
    "                                                    y = f(X)\n",
    "                                                    \n",
    "### Regression : \n",
    "A supervised learning problem where we try to map input variable (X) to continuous output (y). For example : Trying to predict the house price on the basis of number of rooms, built area etc.\n",
    "\n",
    "### Classification :\n",
    "A supervised learning problem where we try to map input variable (X) to discrete output (y). For example : Marking mails or SMSs as spam/not-spam on the basis of its content, origin etc. \n",
    "\n",
    "## Unsupervised Learning\n",
    "\n",
    "Unsupervised learning is where you only have input data (X) and no corresponding output variables. The goal for unsupervised learning is to model the underlying structure or distribution in the data in order to learn more about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "<img src=\"media/img1.png\" alt=\"Regression\" style=\"width : 500px;\"/>\n",
    "\n",
    "We will mainly cover three topics : \n",
    "1. Hypothesis Function - Defining our function f(x)\n",
    "2. Cost Function - Checking the performance of out model/function.\n",
    "3. Optimization (Gradient Descent) - Improving the performance of our model/function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Function\n",
    "\n",
    "The hypothesis function for univariate linear regression problem is :\n",
    "$$\\widehat{y} = w_{0} + w_{1}x$$\n",
    "\n",
    "$\\widehat{y}$ = Output predicted by our model.\n",
    "\n",
    "$w_{0} , w_{1}$ = Parameters of our model, also known as weights.\n",
    "\n",
    "$x$ = The input feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hypothesis function for multivariate linear regression problem is :\n",
    "\n",
    "$$\\widehat{y} = w_{0} + w_{1}x_{1} + w_{2}x_{2} + .... + w_{m}x_{m}$$\n",
    "\n",
    "$\\widehat{y}$ = Output predicted by our model.\n",
    "\n",
    "$w_{0} , w_{1} , w_{2} , ... , w_{m}$ = Parameters of our model, also known as weights (W).\n",
    "\n",
    "$x_{1} , x_{2} , ... , x_{m}$ = Values of our input feature vector (X).\n",
    "\n",
    "$m$ = Number of features\n",
    "\n",
    "\n",
    "Any set of $W = [w_{0} , w_{1} , w_{2} , ... , w_{m}]$ uniquely identifies a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, in a univariate problem, for a particular value of $w_{0}, w_{1}$, we can have this plot : \n",
    "\n",
    "<img src=\"media/img4.png\" alt=\"Regression\" style=\"width : 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "It is used to check how good or bad our current model is performing.\n",
    "\n",
    "Cost function of our model/function parameterized by $W$,\n",
    "$$ J(W) = \\frac{1}{n} \\sum_{i=1}^{n} (\\widehat{y_{i}} - y_{i})^{2}$$\n",
    "\n",
    "$W = [w_{0},w_{1},w_{2},...w_{m}]$\n",
    "\n",
    "$\\widehat{y_{i}}$ = Output predicted by our model for $i^{th}$ input\n",
    "\n",
    "$y_{i}$ = Actual output for $i^{th}$ input\n",
    "\n",
    "$n$ = Number of data points\n",
    "\n",
    "#### So our ultimate objective is to minimize the cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization (Gradient Descent)\n",
    "\n",
    "If our model is not performing good we need to train it. Training a model means changing/tuning the value of W (weight/parameters of our model), so that the cost decreases. \n",
    "\n",
    "Let us assusme that,\n",
    "$$\\widehat{y} = w_{1}x_{1}$$\n",
    "\n",
    "Hence our cost function becomes : \n",
    "$$ J(w_{1}) = \\frac{1}{n} \\sum_{i=1}^{n} (w_{1}(x_{1}^{(i)}) - y_{i})^{2}$$\n",
    "\n",
    "Plotting $J(w_{1})$ against $w_{1}$:\n",
    "\n",
    "<img src=\"media/img5.png\" alt=\"Regression\" style=\"width : 500px;\"/>\n",
    "\n",
    "In two dimension, the plot would look something like this:\n",
    "\n",
    "<img src=\"media/img6.jpg\" alt=\"Regression\" style=\"width : 500px;\"/>\n",
    "\n",
    "It would look scary in higher dimensions, so lets leave that for now!\n",
    "\n",
    "\n",
    "Lets not forget that our ultimate goal is to minimize the cost $J(W)$. To minimize $J(W)$ we'll perform gradient decent steps iteratively. \n",
    "\n",
    "One step of gradient descent is : \n",
    "\n",
    "For all weights $w_{j}$ :\n",
    "\n",
    "$$w_{j} = w_{j} - \\alpha \\frac{\\partial}{\\partial w_{j}} J(W)$$\n",
    "\n",
    "$\\alpha$ = Learning rate\n",
    "\n",
    "$\\frac{\\partial}{\\partial w_{j}} J(W)$ = Partial derivative of cost function $J(W)$ w.r.t. $w_{i}$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_{j}} J(W) = \\frac{2}{n} \\sum_{i=1}^{n}(\\widehat{y_{i}} - y_{i})x_{j}^{(i)}$$\n",
    "\n",
    "This gradient descent step is done repetitively untill the learning is stopped.\n",
    "\n",
    "<img src=\"media/img7.png\" alt=\"Regression\" style=\"width : 500px; height : 400px;\"/>\n",
    "\n",
    "<img src=\"media/img8.gif\" alt=\"Regression\" style=\"width : 500px;\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
