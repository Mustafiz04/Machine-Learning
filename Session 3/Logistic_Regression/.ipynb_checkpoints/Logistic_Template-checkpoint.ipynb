{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy , matplotlib.pyplot , pandas , seaborn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Name of the dataset is \"Social_Network_Ads.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Q) How many data points and features ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Q) What are the column names in our dataset ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Q) How many datapoints for each class are present ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First five rows of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Stats / Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of Age vs EstimatedSalary (Independent Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%matplotlib\\nsns.set_style(\"whitegrid\");\\nsns.FacetGrid(dataset, hue=\"Purchased\",size=4)     .map(plt.scatter, \"Age\",\"EstimatedSalary\")     .add_legend();\\nplt.show()\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%matplotlib\n",
    "sns.set_style(\"whitegrid\");\n",
    "sns.FacetGrid(dataset, hue=\"Purchased\",size=4) \\\n",
    "    .map(plt.scatter, \"Age\",\"EstimatedSalary\") \\\n",
    "    .add_legend();\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix of features X and Vector of Independent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix of features will contain Age and Estimated salary (Independent Variables)\n",
    "\n",
    "# Vector of Predictions will contain the Dependent variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset into the Training set and Test set\n",
    "\n",
    "A convenient way to randomly partition this dataset into a separate test and training dataset is to use the \n",
    "train_test_split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train  = > Matrix of Features in the training set\n",
    "# X_test   = > Matrix of Features in the test set\n",
    "# y_train  = > Matrix of Features in the training set\n",
    "# X_train  = > Matrix of Features in the test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "\n",
    "  Most of the times, your dataset will contain features highly varying in magnitudes, units and range. But \n",
    "  since, most of the machine learning algorithms use Eucledian distance between two data points in their \n",
    "  computations, this is a problem.\n",
    "\n",
    "  If left alone, these algorithms only take in the magnitude of features neglecting the units. The results \n",
    "  would vary greatly between different units, 5kg and 5000gms. The features with high magnitudes will weigh\n",
    "  in a lot more in the distance calculations than features with low magnitudes.\n",
    "\n",
    "  To suppress this effect, we need to bring all features to the same level of magnitudes. This can be\n",
    "  achieved by scaling.\n",
    "<img src=\"scale.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use StandardScaler Class for Feature Scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Logistic Regression to the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the test set results in a vector of predictions \"y_pred\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Model's Performance\n",
    "\n",
    "## 1) False Positives and False negatives\n",
    "<img src=\"error.png\">\n",
    "### a) False Positives or Type I Errors :\n",
    "  \n",
    "A false positive is an error when the model incorrectly predicts the positive class  ie., the model\n",
    "predicts a positive outcome whereas in reality the outcome is negative.\n",
    "  \n",
    "### b) False Negatives or Type II Errors :\n",
    " A false negative is an error when the model incorrectly predicts the negative class  ie., the model\n",
    " predicts a negative outcome whereas in reality the outcome is positive.\n",
    "\n",
    "## 2) Reading a Confusion Matrix\n",
    " \n",
    "The confusion matrix lays out the performance of a learning algorithm. It is simply a square matrix \n",
    "that reports the counts of the true positive , true negative , false positive and false negative \n",
    "predictions of a classifier as shown in the figure :\n",
    "<img src=\"c.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Accuracy rate and Error rate :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the Accuracy and Error rate of prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the Training set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"plt.close()\\n%matplotlib\\nfrom matplotlib.colors import ListedColormap\\nX_set, y_set = X_train, y_train\\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\\nplt.xlim(X1.min(), X1.max())\\nplt.ylim(X2.min(), X2.max())\\nfor i, j in enumerate(np.unique(y_set)):\\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\\n                c = ListedColormap(('red', 'green'))(i), label = j)\\nplt.title('Logistic Regression (Training set)')\\nplt.xlabel('Age')\\nplt.ylabel('Estimated Salary')\\nplt.legend()\\nplt.show()\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''plt.close()\n",
    "%matplotlib\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_train, y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Logistic Regression (Training set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n%matplotlib\\nfrom matplotlib.colors import ListedColormap\\nX_set, y_set = X_test, y_test\\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\\nplt.xlim(X1.min(), X1.max())\\nplt.ylim(X2.min(), X2.max())\\nfor i, j in enumerate(np.unique(y_set)):\\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\\n                c = ListedColormap(('red', 'green'))(i), label = j)\\nplt.title('Logistic Regression (Test set)')\\nplt.xlabel('Age')\\nplt.ylabel('Estimated Salary')\\nplt.legend()\\nplt.show()\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%matplotlib\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_test, y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Logistic Regression (Test set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
